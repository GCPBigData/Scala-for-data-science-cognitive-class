{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 2: Preparing Data \n",
    "\n",
    "## Identifying Outliers\n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "After completing this lesson, you should be able to:\n",
    "\n",
    "- Compute the inverse of covariance matrix given of a dataset\n",
    "-\tCompute Mahalanobis Distance for all elements in a dataset\n",
    "-\tRemove outliers from a dataset\n",
    "\n",
    "\n",
    "## Mahalanobis Distance \n",
    "\n",
    "-\tMulti-dimensional generalization of measuring how many standard deviations a point is away from the mean\n",
    "-\tMeasured along each Principal Component axis \n",
    "-\tUnitless and scale-invariant \n",
    "-\tTakes into account the correlations of the dataset\n",
    "-\tUsed to detect outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@61f09179"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@680a7047\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.StandardScaler\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.VectorAssembler \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.util.MLUtils\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import  org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import  org.apache.spark.ml.feature.StandardScaler\n",
    "import  org.apache.spark.ml.feature.VectorAssembler \n",
    "import org.apache.spark.mllib.util.MLUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfRandom\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, uniform: double ... 2 more fields]\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_44b7a51c46e8\n",
       "\u001b[36mdfVec\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [id: bigint, uniform: double ... 3 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Continuing from Previous Example \n",
    "\n",
    "val  dfRandom = spark.range(0, 10).select(\"id\").\n",
    " withColumn(\"uniform\", rand(10L)).\n",
    " withColumn(\"normal1\", randn(10L)).\n",
    " withColumn(\"normal2\", randn(11L))\n",
    "\n",
    "val  assembler = new VectorAssembler().\n",
    " setInputCols(Array(\"uniform\",\"normal1\",\"normal2\")).\n",
    " setOutputCol(\"features\")\n",
    "\n",
    "val dfVec = MLUtils.convertVectorColumnsFromML(assembler.transform(dfRandom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[0.41371264720975...|\n",
      "|  1|[0.73117192818966...|\n",
      "|  2|[0.19829196382083...|\n",
      "|  3|[0.12714181165849...|\n",
      "|  4|[0.76043181534066...|\n",
      "|  5|[0.12030715258495...|\n",
      "|  6|[0.12131363910425...|\n",
      "|  7|[0.44292918521277...|\n",
      "|  8|[0.88987842538862...|\n",
      "|  9|[0.03650707717266...|\n",
      "+---+--------------------+\n",
      "\n",
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "| 10|       [3.0,3.0,3.0]|\n",
      "|  9|[0.03650707717266...|\n",
      "|  8|[0.88987842538862...|\n",
      "|  7|[0.44292918521277...|\n",
      "|  6|[0.12131363910425...|\n",
      "+---+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfOutlier\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [id: bigint, features: vector]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Continuing from the Previous Example\n",
    "dfVec.select(\"id\",\"features\").show()\n",
    "\n",
    "// An Example with Outliers \n",
    "val dfOutlier = dfVec.select(\"id\",\"features\").unionAll(spark.createDataFrame(Seq((10,Vectors.dense(3, 3, 3)))))\n",
    "dfOutlier.sort(dfOutlier(\"id\").desc).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          scaledFeat|\n",
      "+---+--------------------+\n",
      "| 10|[2.82164986866882...|\n",
      "|  9|[-0.6946827334846...|\n",
      "|  8|[0.31788506121507...|\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mscaler\u001b[39m: \u001b[32mStandardScaler\u001b[39m = stdScal_3799c75835be\n",
       "\u001b[36mscalerModel\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mStandardScalerModel\u001b[39m = stdScal_3799c75835be\n",
       "\u001b[36mdfScaled\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, scaledFeat: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// An Example with Outliers \n",
    "\n",
    "val scaler = new StandardScaler().\n",
    " setInputCol(\"features\").setOutputCol(\"scaledFeat\").\n",
    " setWithStd(true).setWithMean(true)\n",
    "\n",
    "val scalerModel = scaler.fit(MLUtils.convertVectorColumnsToML(dfOutlier.select(\"id\",\"features\")))\n",
    "\n",
    "val dfScaled = scalerModel.transform(MLUtils.convertVectorColumnsToML(dfOutlier)).select(\"id\",\"scaledFeat\")\n",
    "dfScaled.sort(dfScaled(\"id\").desc).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/12/18 12:06:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/12/18 12:06:06 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "19/12/18 12:06:08 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "19/12/18 12:06:08 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.mllib.stat.Statistics\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m breeze.linalg._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrddVec\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mVector\u001b[39m] = MapPartitionsRDD[39] at map at cmd5.sc:5\n",
       "\u001b[36mcolCov\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mMatrix\u001b[39m = 1.0                 0.8103496428958891  0.7305190864811277  \n",
       "0.8103496428958891  1.0                 0.718094148661543   \n",
       "0.7305190864811277  0.718094148661543   1.0                 \n",
       "\u001b[36minvColCovB\u001b[39m: \u001b[32mDenseMatrix\u001b[39m[\u001b[32mDouble\u001b[39m] = 3.358697500406212    -1.9816807132192142  -1.0305593050850421  \n",
       "-1.9816807132192142  3.2338825769455832   -0.8745765716449652  \n",
       "-1.030559305085042   -0.8745765716449652  2.380871560770073    "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  org.apache.spark.mllib.stat.Statistics\n",
    "\n",
    "import  breeze.linalg._\n",
    "\n",
    "val  rddVec = MLUtils.convertVectorColumnsFromML(dfScaled.select(\"scaledFeat\")).rdd.map(_(0).asInstanceOf[org.apache.spark.mllib.linalg.Vector])\n",
    "\n",
    "val  colCov = Statistics.corr(rddVec)\n",
    "val  invColCovB = inv(new DenseMatrix(3, 3, colCov.toArray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmahalanobis\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd6$Helper$$Lambda$4987/1741543454@4fcedc43,\n",
       "  DoubleType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7))\n",
       ")\n",
       "\u001b[36mdfMahalanobis\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, scaledFeat: vector ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Computing Mahalanobis Distance \n",
    "\n",
    "val mahalanobis = udf[Double, org.apache.spark.ml.linalg.Vector]{ v =>\n",
    " val k = v.toArray\n",
    " val vB = new DenseVector(k);\n",
    " vB.t * invColCovB * vB\n",
    "}\n",
    "\n",
    "val dfMahalanobis = dfScaled.withColumn(\"mahalanobis\", mahalanobis(dfScaled(\"scaledFeat\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-------------------+\n",
      "| id|          scaledFeat|        mahalanobis|\n",
      "+---+--------------------+-------------------+\n",
      "|  0|[-0.2471094548651...| 1.5504383991982045|\n",
      "|  1|[0.12957186296999...|  4.167166290100534|\n",
      "|  2|[-0.5027168721611...| 0.5156255322665723|\n",
      "|  3|[-0.5871400869997...|  5.465417156790307|\n",
      "|  4|[0.16429018280530...|0.16147341731596204|\n",
      "|  5|[-0.5952497518954...| 0.8330267386713042|\n",
      "|  6|[-0.5940555052906...|  5.980103636277283|\n",
      "|  7|[-0.2124425709625...| 1.1931413807012845|\n",
      "|  8|[0.31788506121507...|0.28583360788741563|\n",
      "|  9|[-0.6946827334846...| 1.7270411701441382|\n",
      "| 10|[2.82164986866882...|  8.120732670646996|\n",
      "+---+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// Computing Mahalanobis Distance \n",
    "dfMahalanobis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----------------+\n",
      "| id|          scaledFeat|      mahalanobis|\n",
      "+---+--------------------+-----------------+\n",
      "| 10|[2.82164986866882...|8.120732670646996|\n",
      "|  6|[-0.5940555052906...|5.980103636277283|\n",
      "+---+--------------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mids\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [10],\n",
       "  [6],\n",
       "  [3],\n",
       "  [1],\n",
       "  [9],\n",
       "  [0],\n",
       "  [7],\n",
       "  [5],\n",
       "  [2],\n",
       "  [8],\n",
       "  [4]\n",
       ")\n",
       "\u001b[36midOutliers\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mLong\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m10L\u001b[39m, \u001b[32m6L\u001b[39m)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Removing Outliers \n",
    "\n",
    "dfMahalanobis.sort(dfMahalanobis(\"mahalanobis\").desc).show(2)\n",
    "\n",
    "val ids = dfMahalanobis.select(\"id\",\"mahalanobis\").sort(dfMahalanobis(\"mahalanobis\").desc).drop(\"mahalanobis\").collect() \n",
    "\n",
    "val idOutliers = ids.map(_(0).asInstanceOf[Long]).slice(0,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[0.41371264720975...|\n",
      "|  1|[0.73117192818966...|\n",
      "|  3|[0.12714181165849...|\n",
      "|  4|[0.76043181534066...|\n",
      "|  5|[0.12030715258495...|\n",
      "|  6|[0.12131363910425...|\n",
      "|  7|[0.44292918521277...|\n",
      "|  8|[0.88987842538862...|\n",
      "|  9|[0.03650707717266...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfOutlier.filter(\"id not in (10, 2)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "- Having completed this lesson, you should be able to:\n",
    "-\tCompute the inverse of covariance matrix given of a dataset \n",
    "-\tCompute Mahalanobis Distance for all elements in a dataset\n",
    "-\tRemove outliers from a dataset\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
