{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 2: Preparing Data\n",
    "\n",
    "## Statistics, Random data and Sampling on Data Frames\n",
    "\n",
    "### Lesson Objectives \n",
    "\n",
    "After completing this lesson, you should be able to: \n",
    "\n",
    "- Compute column summary statistics\n",
    "-\tCompute pairwise statistics between series/columns\n",
    "-\tPerform standard sampling on any `DataFrame` \n",
    "-\tSplit any `DataFrame` randomly into subsets\n",
    "-\tPerform stratified sampling onto `DataFrames`\n",
    "-\tGenerate Random Data from Uniform and Normal Distributions \n",
    "\n",
    "\n",
    "## Summary Statistics for DataFrames \n",
    "\n",
    "-\tColumn summary statistics for DataFrames are available through DataFrame's `describe()` method\n",
    "-\tIt returns another `DataFrame`, which contains column-wise results for: \n",
    "-\t`min`, `max`\n",
    "-\t`mean`, `stddev`\n",
    "-\t`count`\n",
    "- Column summary statistics can also be computed through DataFrame's `groupBy()` and `agg()` methods, but stddev is not supported\n",
    "-\tIt also returns another `DataFrame` with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@65c263f1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@51209c8\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|  desc|value1|value2|\n",
      "+------+------+------+\n",
      "| first|     1|   3.7|\n",
      "|second|    -2|   2.1|\n",
      "| third|     6|   0.7|\n",
      "+------+------+------+\n",
      "\n",
      "+-------+-----+------------------+------------------+\n",
      "|summary| desc|            value1|            value2|\n",
      "+-------+-----+------------------+------------------+\n",
      "|  count|    3|                 3|                 3|\n",
      "|   mean| null|1.6666666666666667| 2.166666666666667|\n",
      "| stddev| null| 4.041451884327381|1.5011106998930273|\n",
      "|    min|first|                -2|               0.7|\n",
      "|    max|third|                 6|               3.7|\n",
      "+-------+-----+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mRecord\u001b[39m\n",
       "\u001b[36mrecords\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRecord\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[33mRecord\u001b[39m(\u001b[32m\"first\"\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m3.7\u001b[39m),\n",
       "  \u001b[33mRecord\u001b[39m(\u001b[32m\"second\"\u001b[39m, \u001b[32m-2\u001b[39m, \u001b[32m2.1\u001b[39m),\n",
       "  \u001b[33mRecord\u001b[39m(\u001b[32m\"third\"\u001b[39m, \u001b[32m6\u001b[39m, \u001b[32m0.7\u001b[39m)\n",
       ")\n",
       "\u001b[36mrecRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRecord\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd2.sc:5\n",
       "\u001b[36mrecDF\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [desc: string, value1: int ... 1 more field]\n",
       "\u001b[36mrecStats\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [summary: string, desc: string ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Record(desc: String, value1: Int, value2: Double)\n",
    "\n",
    "val records = Array(Record(\"first\",1,3.7), Record(\"second\",-2,2.1), Record(\"third\",6,0.7))\n",
    "\n",
    "val recRDD = sc.parallelize(records)\n",
    "val recDF = spark.createDataFrame(recRDD)\n",
    "recDF.show()\n",
    "\n",
    "val recStats = recDF.describe()\n",
    "recStats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[D@3a91672f\n",
      "[D@48a740e3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3_0\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m = [stddev,null,4.041451884327381,1.5011106998930273]\n",
       "\u001b[36mar1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m4.041451884327381\u001b[39m, \u001b[32m1.5011106998930273\u001b[39m)\n",
       "\u001b[36mar2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m3.0\u001b[39m,\n",
       "  \u001b[32m1.6666666666666667\u001b[39m,\n",
       "  \u001b[32m4.041451884327381\u001b[39m,\n",
       "  \u001b[32m-2.0\u001b[39m,\n",
       "  \u001b[32m6.0\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fetching Results from DataFrame\n",
    "\n",
    "recStats.filter($\"summary\" === \"stddev\").first()\n",
    "  \n",
    "val ar1 = recStats.filter($\"summary\" === \"stddev\").first().toSeq.drop(2).map(_.toString.toDouble).toArray\n",
    "println(ar1)\n",
    " \n",
    "val ar2 = recStats.select(\"value1\").map(s => s(0).toString.toDouble).collect()\n",
    "println(ar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5_0\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m([6])\n",
       "\u001b[36mres5_1\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m([-2,0.7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recDF.groupBy().agg(Map(\"value1\" -> \"min\", \"value1\" -> \"max\" )).collect()\n",
    " \n",
    "recDF.groupBy().agg(Map(\"value1\" -> \"min\", \"value2\" -> \"min\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrecStatsGroup\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [min(value1): int, min(value2): double]\n",
       "\u001b[36mres6_2\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m\"min(value1)\"\u001b[39m, \u001b[32m\"min(value2)\"\u001b[39m)\n",
       "\u001b[36mres6_3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m-2.0\u001b[39m, \u001b[32m0.7\u001b[39m)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val recStatsGroup = recDF.groupBy().agg(min(\"value1\"), min(\"value2\"))\n",
    "\n",
    "recStatsGroup.columns\n",
    " \n",
    "recStatsGroup.first().toSeq.toArray.map(_.toString.toDouble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Statistics on DataFrames \n",
    "\n",
    "-\tMore statistics are available through the stats method in a DataFrame \n",
    "-\tIt returns a `DataFrameStatsFunctions` object, which has the following methods:\n",
    "-\t`corr()` - computes Pearson correlation between two columns\n",
    "-\t`cov()` - computes sample covariance between two columns\n",
    "- `crosstab()` - Computes a pair-wise frequency table of the given columns \n",
    "- `freqItems()` - finds frequent items for columns, possibly with false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(correlation value1 and value2,-0.5879120879120878)\n",
      "(correlation value1 and value2,-3.566666666666667)\n",
      "+----------------+\n",
      "|value1_freqItems|\n",
      "+----------------+\n",
      "|      [-2, 1, 6]|\n",
      "+----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrecDFStat\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataFrameStatFunctions\u001b[39m = org.apache.spark.sql.DataFrameStatFunctions@4400102b"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val  recDFStat = recDF.stat\n",
    "\n",
    "println(\"correlation value1 and value2\",recDFStat.corr(\"value1\", \"value2\"))\n",
    "println(\"correlation value1 and value2\",recDFStat.cov(\"value1\", \"value2\"))\n",
    "recDFStat.freqItems(Seq(\"value1\"), 0.3) .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling on DataFrames \n",
    "\n",
    "-\tCan be performed on any `DataFrame`\n",
    "-\tReturns a sampled subset of a `DataFrame`\n",
    "-\tSampling with or without replacement\n",
    "- Fraction: expected fraction of rows to generate\n",
    "-\tCan be used on bootstrapping procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  1|   20|\n",
      "|  2|   10|\n",
      "|  2|   20|\n",
      "|  2|   30|\n",
      "|  3|   20|\n",
      "|  3|   30|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  1|   20|\n",
      "|  2|   10|\n",
      "|  3|   20|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [key: int, value: int]\n",
       "\u001b[36mdfSampled\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [key: int, value: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val df = spark.createDataFrame(Seq((1, 10), (1, 20), (2, 10), (2, 20), (2, 30), (3, 20), (3, 30))).toDF(\"key\", \"value\")\n",
    "df.show()\n",
    "val dfSampled = df.sample(withReplacement=false, fraction=0.3, seed=11L)\n",
    "dfSampled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Split on DataFrames\n",
    "\n",
    "-\tCan be performed on any DataFrame\n",
    "-\tReturns an array of DataFrames\n",
    "-\tWeighs for the split will be normalized if the do not add up to 1\n",
    "-\tUseful for splitting a data set into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  1|   20|\n",
      "|  2|   10|\n",
      "|  3|   20|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  2|   20|\n",
      "|  2|   30|\n",
      "|  3|   30|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfSplit\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  [key: int, value: int],\n",
       "  [key: int, value: int]\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfSplit = df.randomSplit(weights=Array(0.3, 0.7), seed=11L) \n",
    "\n",
    "dfSplit(0).show()\n",
    "dfSplit(1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling on DataFrames \n",
    "\n",
    "-\tCan be performed on any `DataFrame` \n",
    "- Any column may work as key\n",
    "-\tWithout replacement\n",
    "-\tFraction: specified by key\n",
    "-\tAvailable as `sampleBy` function in `DataFrameStatFunctions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|value|\n",
      "+---+-----+\n",
      "|  1|   10|\n",
      "|  2|   10|\n",
      "|  2|   20|\n",
      "|  3|   30|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.\n",
    "    sampleBy(col=\"key\",\n",
    "    fractions=Map(1 -> 0.7, 2 -> 0.7, 3 -> 0.7),\n",
    "    seed=11L).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Data Generation \n",
    "\n",
    "-\tSQL functions to generate columns filled with random values \n",
    "-\tTwo supported distributions: uniform and normal\n",
    "-\tUseful for randomized algorithms, prototyping and performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+--------------------+\n",
      "| id|            uniform|              normal|\n",
      "+---+-------------------+--------------------+\n",
      "|  0|0.41371264720975787| -0.5877482396744728|\n",
      "|  1| 0.7311719281896606|  1.5746327759749246|\n",
      "|  2| 0.1982919638208397|  -0.256535324205377|\n",
      "|  3|0.12714181165849525|-0.31703264334668824|\n",
      "|  4| 0.7604318153406678|  0.4977629425313746|\n",
      "|  5|0.12030715258495939|  -0.506853671746243|\n",
      "|  6|0.12131363910425985|  1.4250903895905769|\n",
      "|  7|0.44292918521277047| -0.1413699193557902|\n",
      "|  8| 0.8898784253886249|  0.9657665088756656|\n",
      "|  9|0.03650707717266999| -0.5021009082343131|\n",
      "+---+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.sql.functions.{rand, randn}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32mjava\u001b[39m.\u001b[32mlang\u001b[39m.\u001b[32mLong\u001b[39m] = [id: bigint]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import  org.apache.spark.sql.functions.{rand, randn}\n",
    "\n",
    "val df = spark.range(0, 10)\n",
    "\n",
    "df.select(\"id\").withColumn(\"uniform\", rand(10L)).\n",
    "    withColumn(\"normal\", randn(10L)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "-\tHaving completed this lesson, you should be able to:\n",
    "- Compute column summary statistics \n",
    "-\tCompute pairwise statistics between series/columns\n",
    "-\tPerform standard sampling on any `DataFrame` \n",
    "-\tSplit any `DataFrame` randomly into subsets \n",
    "-\tPerform stratified sampling on `DataFrames`\n",
    "-\tGenerate Random Data from Uniform and Normal Distributions\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
