{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 2: Preparing Data - Transformers and Estimators\n",
    "\n",
    "## Transformers and Estimators \n",
    "\n",
    "### Lesson Objectives \n",
    "\n",
    "After completing this lesson, you should be able to: \n",
    "\n",
    "-\tUnderstand, create, and use a `Transformer`\n",
    "-\tUnderstand, create and use an `Estimator` \n",
    "-\tSet parameters of `Transformers` and `Estimators`\n",
    "-\tCreate a feature `Vector` with `VectorAssembler`\n",
    "\n",
    "## Transformers\n",
    "\n",
    "-\tAlgorithm which can transform one `DataFrame` into another `DataFrame`\n",
    "-\tAbstraction that includes feature transformers and learned models. \n",
    "-\tImplements a method `transform(),` which converts one `DataFrame` into another, generally by appending one or more columns\n",
    "-\tInput and output columns set with `setInputCol` and `setOutputCol` methods \n",
    "-\tExamples:\n",
    "  -\tread one or more columns and map them into a new column of feature vectors\n",
    "  -\tread a column containing feature vectors and make a prediction for each vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@19b140cc"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"TransandEstim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.util.MLUtils\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|    1|I wish Java could...|[i, wish, java, c...|\n",
      "|    2|Logistic, regress...|[logistic,, regre...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.{Tokenizer, RegexTokenizer}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6a063d45\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mInt\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m0\u001b[39m, \u001b[32m\"Hi I heard about Spark\"\u001b[39m),\n",
       "  (\u001b[32m1\u001b[39m, \u001b[32m\"I wish Java could use case classes\"\u001b[39m),\n",
       "  (\u001b[32m2\u001b[39m, \u001b[32m\"Logistic, regression, models,are,neat\"\u001b[39m)\n",
       ")\n",
       "\u001b[36msentenceDataFrame\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [label: int, sentence: string]\n",
       "\u001b[36mtokenizer\u001b[39m: \u001b[32mTokenizer\u001b[39m = tok_9af9fca015ec\n",
       "\u001b[36mtokenized\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [label: int, sentence: string ... 1 more field]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  org.apache.spark.ml.feature.{Tokenizer, RegexTokenizer}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "\n",
    "val data = Seq((0, \"Hi I heard about Spark\"), \n",
    "               \n",
    " (1, \"I wish Java could use case classes\"), \n",
    " (2, \"Logistic, regression, models,are,neat\"))\n",
    " \n",
    "val  sentenceDataFrame = spark.createDataFrame(data).toDF(\"label\", \"sentence\")\n",
    "val tokenizer = new Tokenizer(). setInputCol(\"sentence\").setOutputCol(\"words\")\n",
    "val tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimators \n",
    "\n",
    "-\tAlgorithm which can be fit on a `DataFrame` to produce a `Transformer` \n",
    "-\tAbstracts the concept of a learning algorithm or any algorithm that fits or trains on data \n",
    "-\tImplements a method `fit(),` which accepts a `DataFrame` and produces a `Model`, which is a `Transformer`\n",
    "-\tExample: `LogisticRegression`\n",
    "-\tIt is a learning algorithm and therefore an `Estimator` \n",
    "- By calling the method `fit()` to train the logistic regression, a `Model` is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scaal"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/12/18 11:51:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/12/18 11:51:05 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "19/12/18 11:51:05 INFO LBFGS: Step Size: 3.460\n",
      "19/12/18 11:51:05 INFO LBFGS: Val and Grad Norm: 0.542685 (rel: 0.217) 0.957249\n",
      "19/12/18 11:51:05 INFO StrongWolfeLineSearch: Line search t: 0.30462261454465644 fval: 0.4921201099913023 rhs: 0.5426742715671284 cdd: -9.594931767722414E-4\n",
      "19/12/18 11:51:05 INFO LBFGS: Step Size: 0.3046\n",
      "19/12/18 11:51:05 INFO LBFGS: Val and Grad Norm: 0.492120 (rel: 0.0932) 0.247119\n",
      "19/12/18 11:51:05 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:05 INFO LBFGS: Val and Grad Norm: 0.485161 (rel: 0.0141) 0.196266\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.465140 (rel: 0.0413) 0.155534\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.421414 (rel: 0.0940) 0.170850\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.359974 (rel: 0.146) 0.224392\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.320565 (rel: 0.109) 0.234840\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.261039 (rel: 0.186) 0.152661\n",
      "19/12/18 11:51:06 INFO StrongWolfeLineSearch: Line search t: 0.42413942950813466 fval: 0.25694096526464916 rhs: 0.26103846323835567 cdd: 3.7219877836733653E-4\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 0.4241\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.256941 (rel: 0.0157) 0.0467588\n",
      "19/12/18 11:51:06 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:51:06 INFO LBFGS: Val and Grad Norm: 0.256309 (rel: 0.00246) 0.0233555\n",
      "19/12/18 11:51:06 INFO LBFGS: Converged because max iterations reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.classification.LogisticRegression\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.util.MLUtils\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtraining\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [label: double, features: vector]\n",
       "\u001b[36mlr\u001b[39m: \u001b[32mLogisticRegression\u001b[39m = logreg_a63cf048ed24\n",
       "\u001b[36mres3_5\u001b[39m: \u001b[32mLogisticRegression\u001b[39m = logreg_a63cf048ed24\n",
       "\u001b[36mmodel1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionModel\u001b[39m = LogisticRegressionModel: uid = logreg_a63cf048ed24, numClasses = 2, numFeatures = 3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A Simple Example of an Estimator\n",
    "\n",
    "import  org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "\n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "\n",
    "val training = MLUtils.convertVectorColumnsToML(spark.createDataFrame(Seq(\n",
    " (1.0, Vectors.dense(0.0, 1.1, 0.1)),\n",
    " (0.0, Vectors.dense(2.0, 1.0, -1.0)), \n",
    " (0.0, Vectors.dense(20, 1.3, 1.0)),\n",
    " (1.0, Vectors.dense(0.0, 1.2, -0.5)))).toDF(\"label\", \"features\"))\n",
    "\n",
    "val lr = new LogisticRegression() \n",
    "\n",
    "lr.setMaxIter(10).setRegParam(0.01)\n",
    "\n",
    "val model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0| [0.0,1.1,0.1]|[-2.6711500036993...|[0.06469734519861...|       1.0|\n",
      "|  0.0|[2.0,1.0,-1.0]|[1.25786304216765...|[0.77865802305260...|       0.0|\n",
      "|  0.0|[20.0,1.3,1.0]|[2.7855877025948,...|[0.94189202365949...|       0.0|\n",
      "|  1.0|[0.0,1.2,-0.5]|[-1.3897978468983...|[0.19944003140081...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1.transform(training).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "-\tTransformers and Estimators use a uniform API for specifying parameters\n",
    "-\tA `ParamMap` is a set of `(parameter, value)` pairs\n",
    "-\tParameters are specific to a given instance\n",
    "-\tThere are two main ways to pass parameters to an algorithm: \n",
    "  -\tSetting parameters for an instance using an appropriate method, for instance `setMaxIter(10)`\n",
    "  -\tPassing a `ParamMap` to `fit()` or `transform(),` for instance, `ParamMap(lr1.MaxIter->10,lr2.MaxIter->20)`\n",
    "  -\tIn this case, the parameter `MaxIter` is being specified to two different instances of models, `lr1` and `lr2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 3.460\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.542685 (rel: 0.217) 0.957249\n",
      "19/12/18 11:52:47 INFO StrongWolfeLineSearch: Line search t: 0.3046226145446599 fval: 0.49212010999130223 rhs: 0.5426742715671288 cdd: -9.594931767723872E-4\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 0.3046\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.492120 (rel: 0.0932) 0.247119\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.485161 (rel: 0.0141) 0.196266\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.465140 (rel: 0.0413) 0.155534\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.421414 (rel: 0.0940) 0.170850\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.359974 (rel: 0.146) 0.224392\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.320565 (rel: 0.109) 0.234840\n",
      "19/12/18 11:52:47 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:47 INFO LBFGS: Val and Grad Norm: 0.261039 (rel: 0.186) 0.152661\n",
      "19/12/18 11:52:48 INFO StrongWolfeLineSearch: Line search t: 0.4241394295081319 fval: 0.25694096526464916 rhs: 0.261038463238355 cdd: 3.7219877836692453E-4\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 0.4241\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.256941 (rel: 0.0157) 0.0467588\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.256309 (rel: 0.00246) 0.0233555\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.256125 (rel: 0.000716) 0.00777617\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.256089 (rel: 0.000142) 0.0112705\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.255935 (rel: 0.000600) 0.0139769\n",
      "19/12/18 11:52:48 INFO StrongWolfeLineSearch: Line search t: 0.1 fval: 0.2559392149660819 rhs: 0.255935315047925 cdd: 0.002569075087704384\n",
      "19/12/18 11:52:48 INFO StrongWolfeLineSearch: Line search t: 0.04925855554409634 fval: 0.2558740689075849 rhs: 0.25593532766481486 cdd: -1.196606386026644E-10\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 0.04926\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.255874 (rel: 0.000239) 0.0409869\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.255521 (rel: 0.00138) 0.0421880\n",
      "19/12/18 11:52:48 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:48 INFO LBFGS: Val and Grad Norm: 0.252739 (rel: 0.0109) 0.0475318\n",
      "19/12/18 11:52:49 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:49 INFO LBFGS: Val and Grad Norm: 0.243056 (rel: 0.0383) 0.0521965\n",
      "19/12/18 11:52:49 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:49 INFO LBFGS: Val and Grad Norm: 0.229420 (rel: 0.0561) 0.0419548\n",
      "19/12/18 11:52:49 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:49 INFO LBFGS: Val and Grad Norm: 0.219450 (rel: 0.0435) 0.0113495\n",
      "19/12/18 11:52:49 INFO LBFGS: Step Size: 1.000\n",
      "19/12/18 11:52:49 INFO LBFGS: Val and Grad Norm: 0.218633 (rel: 0.00372) 0.0380846\n",
      "19/12/18 11:52:49 INFO LBFGS: Converged because max iterations reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0| [0.0,1.1,0.1]|[-2.0332608460031...|[0.11575473786499...|       1.0|\n",
      "|  0.0|[2.0,1.0,-1.0]|[1.93242469544196...|[0.87351755418175...|       0.0|\n",
      "|  0.0|[20.0,1.3,1.0]|[2.46813565317611...|[0.92187760149913...|       0.0|\n",
      "|  1.0|[0.0,1.2,-0.5]|[-2.5016071722096...|[0.07574558805090...|       1.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.param.ParamMap\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mparamMap\u001b[39m: \u001b[32mParamMap\u001b[39m = {\n",
       "\tlogreg_a63cf048ed24-maxIter: 20,\n",
       "\tlogreg_a63cf048ed24-regParam: 0.01\n",
       "}\n",
       "\u001b[36mmodel2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mclassification\u001b[39m.\u001b[32mLogisticRegressionModel\u001b[39m = LogisticRegressionModel: uid = logreg_a63cf048ed24, numClasses = 2, numFeatures = 3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A Simple Example of Parameter Setting \n",
    "\n",
    "import  org.apache.spark.ml.param.ParamMap\n",
    "\n",
    "val  paramMap = ParamMap(lr.maxIter -> 20, lr.regParam -> 0.01)\n",
    "\n",
    "val model2 = lr.fit(training, paramMap)\n",
    "\n",
    "model2.transform(training).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Assembler\n",
    "\n",
    "-\tTransformer that combines a given list of columns into a single vector column\n",
    "-\tUseful for combining raw features and features generated by other transformers into a single feature vector\n",
    "-\tAccepts the following input column types: \n",
    "  -\tall numeric types \n",
    "  -\tboolean\n",
    "  -\tvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[0.41371264720975...|\n",
      "|  1|[0.73117192818966...|\n",
      "|  2|[0.19829196382083...|\n",
      "|  3|[0.12714181165849...|\n",
      "|  4|[0.76043181534066...|\n",
      "|  5|[0.12030715258495...|\n",
      "|  6|[0.12131363910425...|\n",
      "|  7|[0.44292918521277...|\n",
      "|  8|[0.88987842538862...|\n",
      "|  9|[0.03650707717266...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.VectorAssembler\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.sql.functions._\n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdfRandom\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, uniform: double ... 2 more fields]\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_2e7c92afbd26\n",
       "\u001b[36mdfVec\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, uniform: double ... 3 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// An Example of a VectorAssembler\n",
    "\n",
    "import  org.apache.spark.ml.feature.VectorAssembler\n",
    "import  org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "val dfRandom = spark.range(0, 10).select(\"id\")\n",
    " .withColumn(\"uniform\", rand(10L))\n",
    " .withColumn(\"normal1\", randn(10L))\n",
    " .withColumn(\"normal2\", randn(11L))\n",
    "\n",
    "val assembler = new VectorAssembler().\n",
    " setInputCols(Array(\"uniform\",\"normal1\",\"normal2\")).\n",
    " setOutputCol(\"features\")\n",
    "\n",
    "val dfVec = assembler.transform(dfRandom)\n",
    "\n",
    "\n",
    "// An Example of a VectorAssembler\n",
    "\n",
    "dfVec.select(\"id\",\"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary \n",
    "\n",
    "-\tHaving completed this lesson, you should be able to: \n",
    "  - Understand, create, and use a `Transformer`\n",
    "  -\tUnderstand, create, and use an `Estimator`\n",
    "  -\tSet parameters of `Transformers` and `Estimators`\n",
    "  -\tCreate a feature `Vector` with `VectorAssembler`\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
