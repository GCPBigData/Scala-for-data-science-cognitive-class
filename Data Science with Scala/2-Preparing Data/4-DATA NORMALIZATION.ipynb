{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 2: Preparing Data - Data Normalization\n",
    "\n",
    "## Data Normalization \n",
    "\n",
    "### Lesson Objectives\n",
    "\n",
    "-\tAfter completing this lesson, you should be able to: \n",
    "-\tNormalize a dataset to have unit p-norm\n",
    "-\tNormalize a dataset to have unit standard deviation and zero mean \n",
    "-\tNormalize a dataset to have given minimum and maximum values \n",
    "\n",
    "\n",
    "## Normalizer\n",
    "\n",
    "-\tA Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm\n",
    "-\tTakes a parameter P, which specifies the p-norm used for normalization (p=2 by default)\n",
    "- Standardize input data and improve the behavior of learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@3fa2dbe2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1ef6de3c\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            features|\n",
      "+---+--------------------+\n",
      "|  0|[0.41371264720975...|\n",
      "|  1|[0.73117192818966...|\n",
      "|  2|[0.19829196382083...|\n",
      "|  3|[0.12714181165849...|\n",
      "|  4|[0.76043181534066...|\n",
      "|  5|[0.12030715258495...|\n",
      "|  6|[0.12131363910425...|\n",
      "|  7|[0.44292918521277...|\n",
      "|  8|[0.88987842538862...|\n",
      "|  9|[0.03650707717266...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.VectorAssembler\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.sql.functions._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdfRandom\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, uniform: double ... 2 more fields]\n",
       "\u001b[36massembler\u001b[39m: \u001b[32mVectorAssembler\u001b[39m = vecAssembler_54eefc605323\n",
       "\u001b[36mdfVec\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: bigint, uniform: double ... 3 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Continuing from Previous Example \n",
    "\n",
    "import  org.apache.spark.ml.feature.VectorAssembler\n",
    "\n",
    "import  org.apache.spark.sql.functions._\n",
    "\n",
    "val dfRandom = spark.range(0, 10).select(\"id\").\n",
    " withColumn(\"uniform\", rand(10L)).\n",
    " withColumn(\"normal1\", randn(10L)).\n",
    " withColumn(\"normal2\", randn(11L))\n",
    "\n",
    "val assembler = new  VectorAssembler().\n",
    " setInputCols(Array(\"uniform\",\"normal1\",\"normal2\")).\n",
    " setOutputCol(\"features\")\n",
    "\n",
    "val dfVec = assembler.transform(dfRandom)\n",
    "\n",
    "\n",
    "// Continuing from Previous Example \n",
    "\n",
    "dfVec.select(\"id\",\"features\").show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|          scaledFeat|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[0.41371264720975...|[0.32886636983701...|\n",
      "|  1|[0.73117192818966...|[0.27877135762286...|\n",
      "|  2|[0.19829196382083...|[0.20619308493718...|\n",
      "|  3|[0.12714181165849...|[0.06801701322638...|\n",
      "|  4|[0.76043181534066...|[0.54081735791552...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.Normalizer\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mscaler1\u001b[39m: \u001b[32mNormalizer\u001b[39m = normalizer_aa922c1c6e2e"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// A Simple Normalizer \n",
    "\n",
    "import  org.apache.spark.ml.feature.Normalizer\n",
    "\n",
    "val scaler1 = new Normalizer().setInputCol(\"features\").setOutputCol(\"scaledFeat\").setP(1.0)\n",
    "scaler1.transform(dfVec.select(\"id\",\"features\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "\n",
    "-\tA Model which can be fit on a dataset to produce a `StandardScalerModel`\n",
    "-\tA Transformer which transforms a dataset of `Vector` rows, normalizing each feature to have unit standard deviation and/or zero mean\n",
    "- Takes two parameters:\n",
    "\t-\t`withStd`: scales the data to unit standard deviation (default: true)\n",
    "\t-\t`withMean`: centers the data with mean before scaling (default: false)\n",
    "-\tIt builds a dense output, sparse inputs will raise an exception\n",
    "-\tIf the standard deviation of a feature is zero, it returns 0.0 in the Vector for that feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|          scaledFeat|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[0.41371264720975...|[0.09436171519474...|\n",
      "|  1|[0.73117192818966...|[1.10830425730255...|\n",
      "|  2|[0.19829196382083...|[-0.5936767465057...|\n",
      "|  3|[0.12714181165849...|[-0.8209253118403...|\n",
      "|  4|[0.76043181534066...|[1.20175827215480...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.StandardScaler\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mscaler2\u001b[39m: \u001b[32mStandardScaler\u001b[39m = stdScal_446516dbfc0c\n",
       "\u001b[36mscaler2Model\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mStandardScalerModel\u001b[39m = stdScal_446516dbfc0c"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A Simple Standard Scaler \n",
    "\n",
    "import  org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "val  scaler2 = new StandardScaler().\n",
    " setInputCol(\"features\"). setOutputCol(\"scaledFeat\").\n",
    " setWithStd(true). setWithMean(true)\n",
    "\n",
    "val  scaler2Model = scaler2.fit(dfVec.select(\"id\",\"features\"))\n",
    "scaler2Model.transform(dfVec.select(\"id\",\"features\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax Scaler \n",
    "\n",
    "-\tA Model which can be fit on a dataset to produce a `MinMaxScalerModel`\n",
    "-\tA Transformer which transforms a dataset of `Vector` rows, rescaling each feature to a specific range (often `[0,1]`)\n",
    "-\tTakes two parameters: \n",
    "\t-\tmin: lower bound after transformation, shared by all features (default:0.0)\n",
    "\t-\tmax: upper bound after transformation, shared by all features (default: 1.0)\n",
    "-\tSince zero values are likely to be transformed to non-zero values, sparse inputs may result in dense outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            features|          scaledFeat|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[0.41371264720975...|[-0.1159638278794...|\n",
      "|  1|[0.73117192818966...|[0.62804821715480...|\n",
      "|  2|[0.19829196382083...|[-0.6208335632866...|\n",
      "|  3|[0.12714181165849...|[-0.7875843038899...|\n",
      "|  4|[0.76043181534066...|[0.69662302274718...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.ml.feature.MinMaxScaler \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mscaler3\u001b[39m: \u001b[32mMinMaxScaler\u001b[39m = minMaxScal_c7767985479b\n",
       "\u001b[36mscaler3Model\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mMinMaxScalerModel\u001b[39m = minMaxScal_c7767985479b"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// A Simple MinMax Scaler \n",
    "import  org.apache.spark.ml.feature.MinMaxScaler \n",
    "\n",
    "val scaler3 = new MinMaxScaler().\n",
    " setInputCol(\"features\").setOutputCol(\"scaledFeat\").\n",
    " setMin(-1.0).setMax(1.0)\n",
    "\n",
    "val scaler3Model = scaler3.fit(dfVec.select(\"id\",\"features\"))\n",
    "scaler3Model.transform(dfVec.select(\"id\",\"features\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary \n",
    "\n",
    "-\tHaving completed this lesson, you should be able to: \n",
    "- Normalize a dataset to have unit p-norm\n",
    "-\tNormalize a dataset to have unit standard deviation and zero mean\n",
    "-\tNormalize a dataset to have given minimum and maximum values\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
