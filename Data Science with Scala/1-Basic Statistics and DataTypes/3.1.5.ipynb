{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Basic Statistics and Data Types\n",
    "\n",
    "## Hypothesis Testing \n",
    "\n",
    "## Lesson Objectives \n",
    "\n",
    "After completing this lesson, you should be able to:\n",
    "\n",
    "-\tPerform hypothesis testing for goodness of fit and independence \n",
    "-\tPerform hypothesis testing for equality and probability distributions\n",
    "-\tPerform kernel density estimation \n",
    "\n",
    "## Hypothesis Testing \n",
    "\n",
    "- Used to determine whether a result is statistically significant, that is, whether it occurred by chance or not \n",
    "-\tSupported tests:\n",
    "  -\tPearson's Chi-Squared test for goodness of fit \n",
    "  -\tPearson's Chi-Squared test for independence\n",
    "-\tKolmogorov-Smirnov test for equality of distribution \n",
    "-\tInputs of type `RDD[LabeledPoint]` are also supported, enabling feature selection\n",
    "\n",
    "\n",
    "### Pearson's Chi-Squared Test for Goodness of Fit \n",
    "\n",
    "-\tDetermines whether an observed frequency distribution differs from a given distribution or not \n",
    "-\tRequires an input of type Vector containing the frequencies of the events \n",
    "-\tIt runs against a uniform distribution, if a second vector to test against is not supplied \n",
    "-\tAvailable as `chiSqTest`() function in Statistics \n",
    "\n",
    "\n",
    "\n",
    "### Libraries required for examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@1a375c70"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"Hypotesis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.stat.Statistics\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvec\u001b[39m: \u001b[32mVector\u001b[39m = [0.3,0.2,0.15,0.1,0.1,0.1,0.05]\n",
       "\u001b[36mgoodnessOfFitTestResult\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mtest\u001b[39m.\u001b[32mChiSqTestResult\u001b[39m = Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 6 \n",
       "statistic = 0.295 \n",
       "pValue = 0.999520973435643 \n",
       "No presumption against null hypothesis: observed follows the same distribution as expected..\n",
       "\u001b[36mres1_5\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mtest\u001b[39m.\u001b[32mChiSqTestResult\u001b[39m = Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 6 \n",
       "statistic = 0.295 \n",
       "pValue = 0.999520973435643 \n",
       "No presumption against null hypothesis: observed follows the same distribution as expected.."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
    "\n",
    "import org.apache.spark.mllib.stat.Statistics\n",
    "\n",
    "val vec: Vector = Vectors.dense(0.3, 0.2, 0.15, 0.1, 0.1, 0.1, 0.05)\n",
    "\n",
    "val goodnessOfFitTestResult = Statistics.chiSqTest(vec)\n",
    "\n",
    "goodnessOfFitTestResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson's Chi-Squared Test for Independence\n",
    "\n",
    "-\tDetermines whether unpaired observations on two variables are independent of each other \n",
    "-\tRequires an input of type Matrix, representing a contingency table, or an `RDD[LabeledPoint]`\n",
    "-\tAvailable as `chiSqTest()` function in Statistics \n",
    "-\tMay be used for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.stat.Statistics \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mmat\u001b[39m: \u001b[32mMatrix\u001b[39m = 13.0  80.0  \n",
       "47.0  11.0  \n",
       "40.0  9.0   \n",
       "\u001b[36mindependenceTestResult\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mtest\u001b[39m.\u001b[32mChiSqTestResult\u001b[39m = Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 2 \n",
       "statistic = 90.22588968846716 \n",
       "pValue = 0.0 \n",
       "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
       "\u001b[36mres2_5\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mtest\u001b[39m.\u001b[32mChiSqTestResult\u001b[39m = Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 2 \n",
       "statistic = 90.22588968846716 \n",
       "pValue = 0.0 \n",
       "Very strong presumption against null hypothesis: the occurrence of the outcomes is statistically independent.."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Testing for Independence \n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
    "import org.apache.spark.mllib.stat.Statistics \n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val mat: Matrix = Matrices.dense(3, 2,\n",
    "Array(13.0, 47.0, 40.0, 80.0, 11.0, 9.0))\n",
    "\n",
    "val independenceTestResult = Statistics.chiSqTest(mat)\n",
    "independenceTestResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.regression.LabeledPoint\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.stat.test.ChiSqTestResult\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mobs\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mLabeledPoint\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd3.sc:4\n",
       "\u001b[36mfeatureTestResults\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mChiSqTestResult\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 1 \n",
       "statistic = 0.75 \n",
       "pValue = 0.3864762307712326 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..,\n",
       "  Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 2 \n",
       "statistic = 3.0000000000000004 \n",
       "pValue = 0.22313016014843035 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
       ")\n",
       "\u001b[36mres3_4\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mChiSqTestResult\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 1 \n",
       "statistic = 0.75 \n",
       "pValue = 0.3864762307712326 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..,\n",
       "  Chi squared test summary:\n",
       "method: pearson\n",
       "degrees of freedom = 2 \n",
       "statistic = 3.0000000000000004 \n",
       "pValue = 0.22313016014843035 \n",
       "No presumption against null hypothesis: the occurrence of the outcomes is statistically independent..\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.mllib.stat.test.ChiSqTestResult\n",
    "\n",
    "val obs: RDD[LabeledPoint] = sc.parallelize(Array(\n",
    "    LabeledPoint(0, Vectors.dense(1.0, 2.0)),\n",
    "    LabeledPoint(0, Vectors.dense(0.5, 1.5)),\n",
    "    LabeledPoint(1, Vectors.dense(1.0, 8.0))))\n",
    "\n",
    "val featureTestResults: Array[ChiSqTestResult] = Statistics.chiSqTest(obs)\n",
    "featureTestResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov Test\n",
    "\n",
    "-\tDetermines whether nor not two probability distributions are equal \n",
    "-\tOne sample, two sided test \n",
    "-\tSupported distributions to test against:\n",
    "-\tnormal distribution (distName='norm')\n",
    "- customized cumulative density function (CDF)\n",
    "-\tAvailable as `kolmogorovSmirnovTest()` function in Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.random.RandomRDDs.normalRDD\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = RandomRDD[5] at RDD at RandomRDD.scala:42\n",
       "\u001b[36mtestResult\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mtest\u001b[39m.\u001b[32mKolmogorovSmirnovTestResult\u001b[39m = Kolmogorov-Smirnov test summary:\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.12019890461912125 \n",
       "pValue = 0.10230385223938121 \n",
       "No presumption against null hypothesis: Sample follows theoretical distribution.\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.random.RandomRDDs.uniformRDD\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdata1\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = RandomRDD[10] at RDD at RandomRDD.scala:42\n",
       "\u001b[36mtestResult1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mmllib\u001b[39m.\u001b[32mstat\u001b[39m.\u001b[32mtest\u001b[39m.\u001b[32mKolmogorovSmirnovTestResult\u001b[39m = Kolmogorov-Smirnov test summary:\n",
       "degrees of freedom = 0 \n",
       "statistic = 0.5022419691869352 \n",
       "pValue = -2.220446049250313E-16 \n",
       "Very strong presumption against null hypothesis: Sample follows theoretical distribution."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Test for Equality of Distribution\n",
    "import org.apache.spark.mllib.random.RandomRDDs.normalRDD\n",
    "\n",
    "val data: RDD[Double] = normalRDD(sc, size=100, numPartitions=1, seed=13L)\n",
    "\n",
    "val testResult = Statistics.kolmogorovSmirnovTest(data, \"norm\", 0, 1)\n",
    " \n",
    "// Test for Equality of Distribution \n",
    "\n",
    "import org.apache.spark.mllib.random.RandomRDDs.uniformRDD\n",
    "\n",
    "val data1: RDD[Double] = uniformRDD(sc, size = 100, numPartitions=1, seed=13L)\n",
    "\n",
    "val testResult1 = Statistics.kolmogorovSmirnovTest(data1, \"norm\", 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Density Estimation \n",
    "\n",
    "-\tComputes an estimate of the probability density function of a random variable, evaluated at a given set of points \n",
    "-\tDoes not require assumptions about the particular distribution that the observed samples are drawn from \n",
    "-\tRequires an RDD of samples\n",
    "-\tAvailable as `estimate()` function in KernelDensity\n",
    "-\tIn Spark, only Gaussian kernel is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/12/17 11:47:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/12/17 11:47:49 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.stat.KernelDensity\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = RandomRDD[15] at RDD at RandomRDD.scala:42\n",
       "\u001b[36mkd\u001b[39m: \u001b[32mKernelDensity\u001b[39m = org.apache.spark.mllib.stat.KernelDensity@44fe8986\n",
       "\u001b[36mdensities\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.13251324189510227\u001b[39m,\n",
       "  \u001b[32m0.2343205768786857\u001b[39m,\n",
       "  \u001b[32m0.37436865774453676\u001b[39m,\n",
       "  \u001b[32m0.2597908788293575\u001b[39m,\n",
       "  \u001b[32m0.11549809683090305\u001b[39m\n",
       ")\n",
       "\u001b[36mres5_4\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.13251324189510227\u001b[39m,\n",
       "  \u001b[32m0.2343205768786857\u001b[39m,\n",
       "  \u001b[32m0.37436865774453676\u001b[39m,\n",
       "  \u001b[32m0.2597908788293575\u001b[39m,\n",
       "  \u001b[32m0.11549809683090305\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Kernel Density Estimation I\n",
    "\n",
    "import org.apache.spark.mllib.stat.KernelDensity\n",
    "\n",
    "val data: RDD[Double] = normalRDD(sc, size=1000, numPartitions=1, seed=17L)\n",
    "\n",
    "val kd = new KernelDensity().setSample(data).setBandwidth(0.1)\n",
    "\n",
    "val densities = kd.estimate(Array(-1.5, -1, -0.5, 1, 1.5))\n",
    "\n",
    "densities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = RandomRDD[17] at RDD at RandomRDD.scala:42\n",
       "\u001b[36mkd\u001b[39m: \u001b[32mKernelDensity\u001b[39m = org.apache.spark.mllib.stat.KernelDensity@66fa75c4\n",
       "\u001b[36mdensities\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.005891454217755318\u001b[39m,\n",
       "  \u001b[32m1.0011358547494325\u001b[39m,\n",
       "  \u001b[32m1.0157407141249963\u001b[39m,\n",
       "  \u001b[32m0.9352095006986689\u001b[39m,\n",
       "  \u001b[32m0.006607054892779689\u001b[39m\n",
       ")\n",
       "\u001b[36mres7_3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m0.005891454217755318\u001b[39m,\n",
       "  \u001b[32m1.0011358547494325\u001b[39m,\n",
       "  \u001b[32m1.0157407141249963\u001b[39m,\n",
       "  \u001b[32m0.9352095006986689\u001b[39m,\n",
       "  \u001b[32m0.006607054892779689\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Kernel Density Estimation II \n",
    "\n",
    "val data: RDD[Double] = uniformRDD(sc, size=1000, numPartitions=1, seed=17L)\n",
    "\n",
    "val kd = new KernelDensity().setSample(data).setBandwidth(0.1)\n",
    "\n",
    "val densities = kd.estimate(Array(-0.25, 0.25, 0.5, 0.75, 1.25))\n",
    "\n",
    "densities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "-\tHaving completed this lesson, you should be able to:\n",
    "- Perform hypothesis testing for goodness of fit and independence \n",
    "-\tPerform hypothesis testing for equality of probability distributions \n",
    "-\tPerform kernel density estimation\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
