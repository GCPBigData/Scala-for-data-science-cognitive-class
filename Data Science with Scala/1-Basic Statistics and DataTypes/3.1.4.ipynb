{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Basic Statistics and Data Types - Sampling \n",
    " \n",
    "\n",
    "## Lesson Objectives\n",
    "\n",
    "-\tAfter completing this lesson, you should be able to:\n",
    "-\tPerform standard sampling on any RDD \n",
    "-\tSplit any RDD randomly into subsets \n",
    "-\tPerform stratified sampling on RDDs of key-value pairs \n",
    "\n",
    "\n",
    "## Sampling \n",
    "\n",
    "-\tCan be performed on any RDD \n",
    "- Returns a sampled subset of an RDD\n",
    "-\tSampling with or without replacement\n",
    "- Fraction:\n",
    "-\twithout replacement - expected size of the sample as fraction of RDD's size \n",
    "-\twith replacement - expected number of times each element is chosen\n",
    "-\tCan be used on bootstrapping procedures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@48a88fda"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"Sampling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36melements\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mVector\u001b[39m] = ParallelCollectionRDD[22] at parallelize at cmd16.sc:4\n",
       "\u001b[36mres16_3\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mVector\u001b[39m] = \u001b[33mArray\u001b[39m([4.0,7.0,13.0], [3.0,-11.0,19.0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// A Simple Sampling \n",
    "\n",
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "val elements: RDD[Vector] = sc.parallelize(Array(\n",
    "    Vectors.dense(4.0,7.0,13.0),\n",
    "    Vectors.dense(-2.0,8.0,4.0),\n",
    "    Vectors.dense(3.0,-11.0,19.0)))\n",
    "\n",
    "elements.sample(withReplacement=false, fraction=0.5, seed=10L).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres17\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mVector\u001b[39m] = \u001b[33mArray\u001b[39m([4.0,7.0,13.0], [3.0,-11.0,19.0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.sample(withReplacement=false, fraction=0.5, seed=7L).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres18\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mVector\u001b[39m] = \u001b[33mArray\u001b[39m([4.0,7.0,13.0], [-2.0,8.0,4.0], [3.0,-11.0,19.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elements.sample(withReplacement=false, fraction=0.5, seed=64L).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Split\n",
    "\n",
    "-\tCan be performed on any RDD\n",
    "-\tReturns an array of RDDs\n",
    "- Weights for the split will be normalized if they do not add up to 1\n",
    "-\tUseful for splitting a data set into training, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = ParallelCollectionRDD[26] at parallelize at cmd19.sc:1\n",
       "\u001b[36msplits\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m]] = \u001b[33mArray\u001b[39m(\n",
       "  MapPartitionsRDD[27] at randomSplit at cmd19.sc:2,\n",
       "  MapPartitionsRDD[28] at randomSplit at cmd19.sc:2,\n",
       "  MapPartitionsRDD[29] at randomSplit at cmd19.sc:2\n",
       ")\n",
       "\u001b[36mtraining\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = MapPartitionsRDD[27] at randomSplit at cmd19.sc:2\n",
       "\u001b[36mtest\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = MapPartitionsRDD[28] at randomSplit at cmd19.sc:2\n",
       "\u001b[36mvalidation\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mInt\u001b[39m] = MapPartitionsRDD[29] at randomSplit at cmd19.sc:2\n",
       "\u001b[36mres19_5\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mLong\u001b[39m] = \u001b[33mArray\u001b[39m(\u001b[32m601116L\u001b[39m, \u001b[32m199882L\u001b[39m, \u001b[32m199002L\u001b[39m)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = sc.parallelize(1 to 1000000)\n",
    "val splits = data.randomSplit(Array(0.6, 0.2, 0.2), seed = 13L)\n",
    "\n",
    "val training = splits(0)\n",
    "val test = splits(1)\n",
    "val validation = splits(2)\n",
    "\n",
    "splits.map(_.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Sampling \n",
    "\n",
    "-\tCan be performed on RDDs of key-value pairs \n",
    "-\tThink of keys as labels and values as an specific attribute\n",
    "- Two supported methods defined in `PairRDDFunctions`:\n",
    "-\t`sampleByKey` requires only one pass over the data and provides an expected sample size\n",
    "-\t`sampleByKeyExact` provides the exact sampling size with 99.99% confidence but requires significantly more resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.distributed.IndexedRow\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrows\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mIndexedRow\u001b[39m] = ParallelCollectionRDD[45] at parallelize at cmd24.sc:4\n",
       "\u001b[36mfractions\u001b[39m: \u001b[32mMap\u001b[39m[\u001b[32mLong\u001b[39m, \u001b[32mDouble\u001b[39m] = \u001b[33mMap\u001b[39m(\u001b[32m0L\u001b[39m -> \u001b[32m1.0\u001b[39m, \u001b[32m1L\u001b[39m -> \u001b[32m0.5\u001b[39m)\n",
       "\u001b[36mapproxSample\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mVector\u001b[39m)] = MapPartitionsRDD[47] at sampleByKey at cmd24.sc:13\n",
       "\u001b[36mres24_5\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mVector\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m0L\u001b[39m, [1.0,2.0]),\n",
       "  (\u001b[32m1L\u001b[39m, [4.0,5.0]),\n",
       "  (\u001b[32m1L\u001b[39m, [7.0,8.0])\n",
       ")\n",
       "\u001b[36mapproxSample2\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mVector\u001b[39m)] = MapPartitionsRDD[50] at sampleByKeyExact at cmd24.sc:19\n",
       "\u001b[36mres24_7\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mLong\u001b[39m, \u001b[32mVector\u001b[39m)] = \u001b[33mArray\u001b[39m((\u001b[32m0L\u001b[39m, [1.0,2.0]), (\u001b[32m1L\u001b[39m, [7.0,8.0]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.linalg.distributed.IndexedRow\n",
    "\n",
    "val rows: RDD[IndexedRow] = sc.parallelize(Array(\n",
    "    IndexedRow(0, Vectors.dense(1.0,2.0)),\n",
    "    IndexedRow(1, Vectors.dense(4.0,5.0)),\n",
    "    IndexedRow(1, Vectors.dense(7.0,8.0))))\n",
    "\n",
    "val fractions: Map[Long, Double] = Map(0L -> 1.0, 1L -> 0.5)\n",
    "\n",
    "val approxSample = rows.map{\n",
    "    case IndexedRow(index, vec) => (index, vec)\n",
    "}.sampleByKey(withReplacement = false, fractions, 9L)\n",
    "\n",
    "approxSample.collect()\n",
    "\n",
    "val approxSample2 = rows.map{\n",
    "    case IndexedRow(index, vec) => (index, vec)\n",
    "}.sampleByKeyExact(withReplacement = false, fractions, 9L)\n",
    "\n",
    "approxSample2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary: \n",
    "\n",
    "-\tHaving completed this lesson, you should now be able to:\n",
    "-\tPerform standard sampling on any RDD\n",
    "-\tSplit any RDD randomly into subsets\n",
    "-\tPerform stratified sampling on RDDs of key-value pairs\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
