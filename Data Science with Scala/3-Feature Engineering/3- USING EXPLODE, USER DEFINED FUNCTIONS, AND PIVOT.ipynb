{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Module 3: Feature Engineering\n",
    "\n",
    "## Using Explode, User Defined Functions, and Pivot\n",
    "\n",
    "\n",
    "### Lesson Objectives: \n",
    "\n",
    "- After completing this lesson, you should be able to use these methods on DataFrames:\n",
    "-\t`explode()`\n",
    "-\tUser Defined Functions \n",
    "-\t`pivot()`\n",
    "\n",
    "\n",
    "\n",
    "### How Can We Turn Sales Data Into A Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@3ae8fd5d"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"UDFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3fe91036\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mSales\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Setting Up the Example\n",
    "\n",
    "case class Sales( id: Int, account: String, year: String, commission: Int, sales_reps: Seq[String])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+-------------+\n",
      "| id|account|year|commission|   sales_reps|\n",
      "+---+-------+----+----------+-------------+\n",
      "|  1|   Acme|2013|      1000|   [Jim, Tom]|\n",
      "|  2|  Lumos|2013|      1100|  [Fred, Ann]|\n",
      "|  3|   Acme|2014|      2800|        [Jim]|\n",
      "|  4|  Lumos|2014|      4200|[Fred, Sally]|\n",
      "+---+-------+----+----------+-------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msales\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: int, account: string ... 3 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sales = spark.createDataFrame(Seq(\n",
    "  Sales(1, \"Acme\", \"2013\", 1000, Seq(\"Jim\", \"Tom\")),\n",
    "  Sales(2, \"Lumos\", \"2013\", 1100, Seq(\"Fred\", \"Ann\")),\n",
    "  Sales(3, \"Acme\", \"2014\", 2800, Seq(\"Jim\")),\n",
    "  Sales(4, \"Lumos\", \"2014\", 4200, Seq(\"Fred\", \"Sally\"))\n",
    "))\n",
    "sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+---------+\n",
      "| id|account|year|commission|sales_rep|\n",
      "+---+-------+----+----------+---------+\n",
      "|  1|   Acme|2013|      1000|      Jim|\n",
      "|  1|   Acme|2013|      1000|      Tom|\n",
      "|  2|  Lumos|2013|      1100|     Fred|\n",
      "|  2|  Lumos|2013|      1100|      Ann|\n",
      "|  3|   Acme|2014|      2800|      Jim|\n",
      "|  4|  Lumos|2014|      4200|     Fred|\n",
      "|  4|  Lumos|2014|      4200|    Sally|\n",
      "+---+-------+----+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.select( $\"id\", $\"account\", $\"year\", $\"commission\", explode($\"sales_reps\") as(\"sales_rep\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlen\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] => \u001b[32mInt\u001b[39m = ammonite.$sess.cmd8$Helper$$Lambda$4803/1803636306@1937dccc\n",
       "\u001b[36mcolumn_len\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(\n",
       "  ammonite.$sess.cmd8$Helper$$Lambda$4803/1803636306@1937dccc,\n",
       "  IntegerType,\n",
       "  \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(\u001b[33mArrayType\u001b[39m(StringType, true)))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val len: (Seq[String] => Int) = (arg: Seq[String]) => {arg.length}\n",
    "val column_len = udf(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+----------+------+---------+\n",
      "| id|account|year|commission| share|sales_rep|\n",
      "+---+-------+----+----------+------+---------+\n",
      "|  1|   Acme|2013|      1000| 500.0|      Jim|\n",
      "|  1|   Acme|2013|      1000| 500.0|      Tom|\n",
      "|  2|  Lumos|2013|      1100| 550.0|     Fred|\n",
      "|  2|  Lumos|2013|      1100| 550.0|      Ann|\n",
      "|  3|   Acme|2014|      2800|2800.0|      Jim|\n",
      "|  4|  Lumos|2014|      4200|2100.0|     Fred|\n",
      "|  4|  Lumos|2014|      4200|2100.0|    Sally|\n",
      "+---+-------+----+----------+------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mexploded\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [id: int, account: string ... 4 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val exploded = sales.select( $\"id\", $\"account\", $\"year\", $\"commission\", ($\"commission\" / column_len($\"sales_reps\")).as(\"share\"), explode($\"sales_reps\").as(\"sales_rep\"))\n",
    "exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pivot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres10\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [Ann,550.0,null],\n",
       "  [Fred,550.0,2100.0],\n",
       "  [Jim,500.0,2800.0],\n",
       "  [Sally,null,2100.0],\n",
       "  [Tom,500.0,null]\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded.groupBy($\"sales_rep\").pivot(\"year\").agg(sum(\"share\")).orderBy(\"sales_rep\").collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres11\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  [Acme,Jim,500.0,2800.0],\n",
       "  [Acme,Tom,500.0,null],\n",
       "  [Lumos,Ann,550.0,null],\n",
       "  [Lumos,Fred,550.0,2100.0],\n",
       "  [Lumos,Sally,null,2100.0]\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exploded.groupBy($\"account\", $\"sales_rep\").pivot(\"year\").agg(sum(\"share\")).orderBy(\"account\", \"sales_rep\").collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Summary\n",
    "\n",
    "Having completed this lesson, you should understand the role of\n",
    "\n",
    "-\t`explode()`\n",
    "-\tUser Defined Functions \n",
    "-\t`pivot()`\n",
    "\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
