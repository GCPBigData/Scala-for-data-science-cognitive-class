{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# Basic Statistics and Data Types - Summary Statistics, Correlations, and Random Data \n",
    "\n",
    "## Summary Statistics, Correlations, and Random Data \n",
    "\n",
    "## Lesson Objectives \n",
    "\n",
    "- After completing this lesson, you should be able to:\n",
    "- Compute column summary statistics\n",
    "- Compute pairwise correlations between series/columns\n",
    "- Generate random data from different distributions \n",
    "\n",
    "\n",
    "## Summary Statistics \n",
    "- Column summary statistics for an instance of `RDD[Vector]` are available through the `colStats()` function in Statistics \n",
    "-\tIt returns an instance of `MultivariateStatisticalSummary`, which contains column-wise results for: \n",
    "-\t`min`, `max`\n",
    "-\t`mean`, `variance` \n",
    "-\t`numNonzeros`\n",
    "-\t`normL1`, `normL2`\n",
    "-\t`count` returns the total count of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@7e9b741e"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"Summary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mean,[4.0,5.0])\n",
      "(variance,[9.0,9.0])\n",
      "(Nonzeros,[3.0,3.0])\n",
      "(L1 norm,[12.0,15.0])\n",
      "(L2 norm,[8.12403840463596,9.643650760992955])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Vector, Vectors}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.stat.Statistics \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.stat.MultivariateStatisticalSummary\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mobservations\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mVector\u001b[39m] = ParallelCollectionRDD[0] at parallelize at cmd3.sc:7\n",
       "\u001b[36msummary\u001b[39m: \u001b[32mMultivariateStatisticalSummary\u001b[39m = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@42ae2189"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.mllib.stat.Statistics \n",
    "import org.apache.spark.mllib.stat.MultivariateStatisticalSummary\n",
    "\n",
    "val observations: RDD[Vector] = sc.parallelize(Array(\n",
    "    Vectors.dense(1.0, 2.0), \n",
    "    Vectors.dense(4.0, 5.0), \n",
    "    Vectors.dense(7.0, 8.0)))\n",
    "    \n",
    "val summary: MultivariateStatisticalSummary = Statistics.colStats(observations)\n",
    "\n",
    "println(\"mean\",summary.mean)\n",
    "\n",
    "println(\"variance\",  summary.variance)\n",
    "\n",
    "println(\"Nonzeros\",summary.numNonzeros)\n",
    "println(\"L1 norm\",summary.normL1)\n",
    "println(\"L2 norm\",summary.normL2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations\n",
    "\n",
    "-\tPairwise correlations among series is available through the `corr()` function in Statistics \n",
    "-\tCorrelation methods supported: \n",
    "-\tPearson (default)\n",
    "-\t`Spearman` (used for rank variables)\n",
    "-\tInputs supported: \n",
    "-\ttwo `RDD[Double]`s, returning a single Double value\n",
    "-\tan `RDD[Vector],` returning a correlation Matrix \n",
    "\n",
    "\n",
    "## Pearson Correlation Between Two Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/12/17 11:18:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/12/17 11:18:15 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mx\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = ParallelCollectionRDD[2] at parallelize at cmd4.sc:1\n",
       "\u001b[36my\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = ParallelCollectionRDD[3] at parallelize at cmd4.sc:2\n",
       "\u001b[36mcorrelation\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m-0.5610408535732833\u001b[39m\n",
       "\u001b[36mres4_3\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m-0.5610408535732833\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val x: RDD[Double] = sc.parallelize(Array(2.0, 9.0, -7.0))\n",
    "val y: RDD[Double]= sc.parallelize(Array(1.0, 3.0, 5.0))\n",
    "val correlation: Double = Statistics.corr(x, y, \"pearson\")\n",
    "\n",
    "correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mVector\u001b[39m] = ParallelCollectionRDD[8] at parallelize at cmd5.sc:1\n",
       "\u001b[36mcorrelMatrix\u001b[39m: \u001b[32mMatrix\u001b[39m = 1.0                  0.05241424183609593  -0.6449020216370243  \n",
       "0.05241424183609593  1.0                  -0.7970167702187486  \n",
       "-0.6449020216370243  -0.7970167702187486  1.0                  \n",
       "\u001b[36mres5_2\u001b[39m: \u001b[32mMatrix\u001b[39m = 1.0                  0.05241424183609593  -0.6449020216370243  \n",
       "0.05241424183609593  1.0                  -0.7970167702187486  \n",
       "-0.6449020216370243  -0.7970167702187486  1.0                  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Pearson Correlation among Series\n",
    "\n",
    "val  data: RDD[Vector] = sc.parallelize(Array(\n",
    "    Vectors.dense(2.0, 9.0, -7.0),\n",
    "    Vectors.dense(1.0, -3.0, 5.0), \n",
    "    Vectors.dense(4.0, 0.0, -5.0)))\n",
    "\n",
    "val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n",
    "\n",
    "correlMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson\n",
      "1.0                 1.0000000000000002  0.8485552916276634  \n",
      "1.0000000000000002  1.0                 0.8485552916276634  \n",
      "0.8485552916276634  0.8485552916276634  1.0                 \n",
      "Spearman\n",
      "1.0  1.0  1.0  \n",
      "1.0  1.0  1.0  \n",
      "1.0  1.0  1.0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mranks\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mVector\u001b[39m] = ParallelCollectionRDD[11] at parallelize at cmd6.sc:1\n",
       "\u001b[36mcorrPearsonMatrix\u001b[39m: \u001b[32mMatrix\u001b[39m = 1.0                 1.0000000000000002  0.8485552916276634  \n",
       "1.0000000000000002  1.0                 0.8485552916276634  \n",
       "0.8485552916276634  0.8485552916276634  1.0                 \n",
       "\u001b[36mcorrSpearmanMatrix\u001b[39m: \u001b[32mMatrix\u001b[39m = 1.0  1.0  1.0  \n",
       "1.0  1.0  1.0  \n",
       "1.0  1.0  1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "// Pearson vs Spearman Correlation among Series \n",
    "\n",
    "val ranks: RDD[Vector] = sc.parallelize(Array(Vectors.dense(1.0,2.0,3.0),\n",
    "    Vectors.dense(5.0,6.0,4.0), Vectors.dense(7.0,8.0,9.0)))\n",
    "\n",
    "val corrPearsonMatrix: Matrix = Statistics.corr(ranks, \"pearson\")\n",
    "println(\"Pearson\")\n",
    "println(corrPearsonMatrix)\n",
    "\n",
    "val corrSpearmanMatrix: Matrix = Statistics.corr(ranks, \"spearman\")\n",
    "println(\"Spearman\")\n",
    "println(corrSpearmanMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Data Generation\n",
    "\n",
    "-\tRandomRDDs generate either random double RDDs or vector RDDs\n",
    "-\tSupported distributions: \n",
    "-\tuniform, normal, lognormal, poisson, exponential, and gamma \n",
    "-\tUseful for randomized algorithms, prototyping and performance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mean,0.999507000000003)\n",
      "(variance,0.9988727569510001)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.random.RandomRDDs._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mmillion\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = RandomRDD[25] at RDD at RandomRDD.scala:42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Simple Example \n",
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "\n",
    "val million = poissonRDD(sc, mean=1.0, size=1000000L, numPartitions=10)\n",
    "\n",
    "println( \"mean\", million.mean )\n",
    "println( \"variance\",million.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(mean,[0.010937196155508687,-8.988235435120678E-4,0.031386402546769276])\n",
      "(variance,[1.0174398462870078,1.0026338296846795,1.0084043745602416])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.random.RandomRDDs._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mVector\u001b[39m] = RandomVectorRDD[38] at RDD at RandomRDD.scala:64\n",
       "\u001b[36mstats\u001b[39m: \u001b[32mMultivariateStatisticalSummary\u001b[39m = org.apache.spark.mllib.stat.MultivariateOnlineSummarizer@695e933c"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Simple Vector Example\n",
    "\n",
    "import org.apache.spark.mllib.random.RandomRDDs._\n",
    "\n",
    "val data = normalVectorRDD(sc, numRows=10000L, numCols=3, numPartitions=10)\n",
    "val stats: MultivariateStatisticalSummary = Statistics.colStats(data)\n",
    "\n",
    "\n",
    "println( \"mean\", stats.mean)\n",
    "println( \"variance\",stats.variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Distributions \n",
    "\t\n",
    "-\t`exponentialRDD` \n",
    "-\t`gammaRDD`\n",
    "-\t`logNormalRDD`\n",
    "- `normalRDD`\n",
    "-\t`poissonRDD`\n",
    "-\t`uniformRDD`\n",
    "-\t`exponentialVectorRDD`\n",
    "-\t`gammaVectorRDD`\n",
    "-\t`logNormalVectorRDD`\n",
    "-\t`normalVectorRDD`\n",
    "-\t`poissonVectorRDD`\n",
    "-\t`uniformVectorRDD`\n",
    "\n",
    "## Lesson Summary \n",
    "\n",
    "-\tHaving completed this lesson, you should now be able to: \n",
    "- Compute column summary statistics \n",
    "-\tCompute pairwise correlations between series/columns \n",
    "-\tGenerate random data from different distributions\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
