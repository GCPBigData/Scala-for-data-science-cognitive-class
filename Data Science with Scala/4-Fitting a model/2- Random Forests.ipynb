{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cocl.us/Data_Science_with_Scalla_top\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0103EN/adds/Data_Science_with_Scalla_notebook_top.png\" width = 750, align = \"center\"></a>\n",
    " <br/>\n",
    "<a><img src=\"https://ibm.box.com/shared/static/ugcqz6ohbvff804xp84y4kqnvvk3bq1g.png\" width=\"200\" align=\"center\"></a>\"\n",
    "\n",
    "# 3.4.2 Random Forests\n",
    "\n",
    "\n",
    "## Lesson Objectives \n",
    "\n",
    "After completing this lesson, you should be able to: \n",
    "\n",
    "* Understand the Pipelines API for Random Forests and Gradient-Boosted Trees\n",
    "* Describe default's Input and Output columns \n",
    "* Perform classification and regression with Random Forests (RFs)\n",
    "* Understand and use Random Forests parameters\n",
    "\n",
    "## Ensemble Method\n",
    "\n",
    "* An Ensemble is a learning algorithm which creates an aggregate model composed of a set of other base models\n",
    "* 'Random Forests' and 'Gradient-Boosted Trees' are ensemble algorithms based on decision trees\n",
    "* Ensemble algorithms are among the top performers for classification and regression problems\n",
    "\n",
    "\n",
    "## Random Forests (RFs)\n",
    "\n",
    "* Random Forests are ensembles of Decision Trees\n",
    "* One of the most successful machine learning models for classification and regression\n",
    "* Random Forests combine many decision trees in order to reduce the risk of overfitting\n",
    "* The Pipelines API for Random Forests supports both binary and multiclass classification\n",
    "* Supports regression\n",
    "* It also supports continuous and categorical features\n",
    "\n",
    "\n",
    "## RF: Basic Algorithm\n",
    "\n",
    "This is a quick description of the basic algorithm of Random Forests:\n",
    "\n",
    "* RF trains a set of decision trees separately while at the same time\n",
    "* RF injects randomness into the training process. This randomness comes from two different sources: \n",
    "  * bootstrapping: subsampling the original data set on each iteration to get a different training set\n",
    "  * considering different random subsets of features to split on at each tree node\n",
    "* Then each tree makes a prediction and the combined predictions from several trees reduces the variance of the predictions and improves the performance on test data\n",
    "  * classification: majority vote - each tree's prediction is counted as a vote for one class and the predicted label is the class with larges number of votes\n",
    "  * regression: average - each tree predicts a real value and the predicted label is equal to the average of all predictions\n",
    "\n",
    "\n",
    "## Random Forest Parameters I\n",
    "\n",
    "Now let's look at the parameters of Random Forests in Spark.ml\n",
    "\n",
    "I start with the most important parameters: the number of trees and the maximum depth which CAN be tuned to improve performance:\n",
    "* **numTrees**: the total number of trees in the forest. As the number of trees increases:\n",
    "  * the variance of prediction decreases, improving test time accuracy\n",
    "  * training time on the other hand increases roughly linearly with the number of trees\n",
    "* **maxDepth**: the maximum depth of each tree in the forest. As trees get deeper:\n",
    "  * model gets more expressive and powerful \n",
    "  * takes longer to train \n",
    "  * more prone to overfitting\n",
    "\n",
    "\n",
    "## Random Forest Parameters II\n",
    "\n",
    "The second set of parameters for Random Forests DO NOT require tuning, but they CAN be tuned to speed up training:\n",
    "* **subsamplingRate**: specifies the fraction of the size of the original data set to be used for training each tree in the forest\n",
    "  * default = 1.0\n",
    "  * This means it uses the entire original data set to subsample\n",
    "  * Decreasing this value can speed up training as it uses a smaller sample, but the accuracy of the model may suffer\n",
    "* **featureSubsetStrategy**: specifies the fraction of total number of features to use as candidates for splitting at each tree node\n",
    "  * decreasing this value can speed up training\n",
    "  * if set too low can also impact the performance\n",
    "\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "The Inputs taken and the Outputs produced by Random Forests in the Pipelines API are, not surprisingly, exactly the same as Decision Trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Param name  | Type(s) | Default    | Description      |\n",
    "| ----------- | ------- | ---------- | ---------------- |\n",
    "| labelCol    | Double  | \"label\"    | Label to predict |\n",
    "| featuresCol | Vector  | \"features\" | Feature vector   |\n",
    "\n",
    "| Param name       | Type(s) | Default         | Description                              | Notes               |\n",
    "| ---------------- | ------- | --------------- | ---------------------------------------- | ------------------- |\n",
    "| predictionCol    | Double  | \"prediction\"    | Predicted label                          |                     |\n",
    "| rawPredictionCol | Vector  | \"rawPrediction\" | Vector of length # classes, with the counts of training instance labels at the tree node which makes the prediction | Classification only |\n",
    "| probabilityCol   | Vector  | \"probability\"   | Vector of length # classes equal to rawPrediction normalized to a multinomial distribution | Classification only |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the base model for Random Forests. A quick recap: as Inputs there are the label and features columns and as Outputs there are the prediction, rawPrediction and probability columns, where the last two only apply for classification trees. \n",
    "\n",
    "\n",
    "## Continuing from Previous Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m org.apache.spark.SparkContext\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@6c22a176"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0` // Or use any other 2.x version here\n",
    "import  org.apache.spark.SparkContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "val sc= new SparkContext(\"local[*]\",\"Decision Trees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\"wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/SC0105EN/data/sample_libsvm_data.txt  -P /resources/data/\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@13bac303\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.util.MLUtils\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder().getOrCreate()\n",
    "import org.apache.spark.mllib.util.MLUtils\n",
    "import org.apache.spark.ml.feature.{StringIndexer, IndexToString, VectorIndexer}\n",
    "import spark.implicits._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [label: double, features: vector]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = MLUtils.loadLibSVMFile(sc, \"data/sample_libsvm_data.txt\").toDF()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classification I\n",
    "\n",
    "Once again I'm going to build up on the previous example: the `DecisionTreeClassifier`. \n",
    "\n",
    "Remember the Pipeline I used then had 4 stages: two preprocessing estimators, one decision tree classifier and one postprocessing transformer. \n",
    "\n",
    "Since I'm using the same training data the only thing I need to change is the classifier itself. All the rest, pre and post processing estimators and transformers, remain the same. \n",
    "\n",
    "So first I create a new instance of a `RandomForestClassifier`. It will take as inputs the columns named `indexedLabel` and `indexedFeatures`. The number of trees I'm going to train is quite small: just 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.util.MLUtils.{\n",
       "  convertVectorColumnsFromML => fromML,\n",
       "  convertVectorColumnsToML => toML\n",
       "}\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.mllib.util.MLUtils.{\n",
    "  convertVectorColumnsFromML => fromML,\n",
    "  convertVectorColumnsToML => toML\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlabelIndexer\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mStringIndexerModel\u001b[39m = strIdx_3d479617b8dd\n",
       "\u001b[36mlabelConverter\u001b[39m: \u001b[32mIndexToString\u001b[39m = idxToStr_6bc51bb8bb4f\n",
       "\u001b[36mfeatureIndexer\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mfeature\u001b[39m.\u001b[32mVectorIndexerModel\u001b[39m = vecIdx_30faa5c0a251\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.RandomForestClassifier\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.classification.RandomForestClassificationModel\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrfC\u001b[39m: \u001b[32mRandomForestClassifier\u001b[39m = rfc_201978c6237b"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(data)\n",
    "val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(toML(data))\n",
    "\n",
    "import org.apache.spark.ml.classification.RandomForestClassifier\n",
    "import org.apache.spark.ml.classification.RandomForestClassificationModel\n",
    "\n",
    "val rfC = new RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setNumTrees(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classification II\n",
    "\n",
    "Then I create a new Pipeline also with 4 stages but replacing the `DecisionTreeClassifier` with the new `RandomForestClassifier` as its third stage.  \n",
    "\n",
    "This is the `pipelineRFC`: the Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.Pipeline\n",
       "\n",
       "// split into training and test data\n",
       "\u001b[39m\n",
       "\u001b[36mtrainingData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [label: double, features: vector]\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [label: double, features: vector]\n",
       "\u001b[36mpipelineRFC\u001b[39m: \u001b[32mPipeline\u001b[39m = pipeline_f93cc524daa7\n",
       "\u001b[36mmodelRFC\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mPipelineModel\u001b[39m = pipeline_f93cc524daa7\n",
       "\u001b[36mpredictionsRFC\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [label: double, features: vector ... 6 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// split into training and test data\n",
    "val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\n",
    "\n",
    "val pipelineRFC = new Pipeline().setStages(Array(labelIndexer, featureIndexer, rfC, labelConverter))\n",
    "val modelRFC = pipelineRFC.fit(toML(trainingData))\n",
    "val predictionsRFC = modelRFC.transform(toML(testData))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the rest is exactly the same as before. Calling the `fit` method to get a model and calling the `transform` method to make predictions. \n",
    "\n",
    "The predictions are then returned in the prediction RFC data frame.\n",
    "\n",
    "## RF Classification III  \n",
    "\n",
    "Let's take a look at the `predictionsRFC` `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|           0.0|  0.0|(692,[124,125,126...|\n",
      "|           0.0|  0.0|(692,[126,127,128...|\n",
      "|           0.0|  0.0|(692,[129,130,131...|\n",
      "|           0.0|  0.0|(692,[152,153,154...|\n",
      "|           0.0|  0.0|(692,[154,155,156...|\n",
      "|           0.0|  0.0|(692,[181,182,183...|\n",
      "|           1.0|  1.0|(692,[123,124,125...|\n",
      "|           1.0|  1.0|(692,[124,125,126...|\n",
      "|           1.0|  1.0|(692,[124,125,126...|\n",
      "|           1.0|  1.0|(692,[125,126,153...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsRFC.select(\"predictedLabel\", \"label\", \"features\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF Classification IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrfModelC\u001b[39m: \u001b[32mRandomForestClassificationModel\u001b[39m = RandomForestClassificationModel (uid=rfc_201978c6237b) with 3 trees"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rfModelC = modelRFC.stages(2).asInstanceOf[RandomForestClassificationModel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mlinalg\u001b[39m.\u001b[32mVector\u001b[39m = (692,[208,295,318,377,429,462,492,512],[0.003100384043954838,0.016931216931216884,0.013877909530083436,0.26759259259259266,0.04880952380952386,0.31369047619047624,0.01964285714285715,0.3163550397592951])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModelC.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive the Random Forest Classification model and from that, see the feature importances.\n",
    "\n",
    "## RF Classification V\n",
    "\n",
    "Now let's take a look at the model's rules. I can use `toDebugString` to inspect the rules of each and every tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification forest model:\n",
      "RandomForestClassificationModel (uid=rfc_201978c6237b) with 3 trees\n",
      "  Tree 0 (weight 1.0):\n",
      "    If (feature 512 <= 1.5)\n",
      "     If (feature 208 <= 214.0)\n",
      "      Predict: 0.0\n",
      "     Else (feature 208 > 214.0)\n",
      "      If (feature 318 <= 6.5)\n",
      "       Predict: 0.0\n",
      "      Else (feature 318 > 6.5)\n",
      "       Predict: 1.0\n",
      "    Else (feature 512 > 1.5)\n",
      "     Predict: 1.0\n",
      "  Tree 1 (weight 1.0):\n",
      "    If (feature 462 <= 62.5)\n",
      "     If (feature 492 <= 205.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 492 > 205.5)\n",
      "      Predict: 0.0\n",
      "    Else (feature 462 > 62.5)\n",
      "     Predict: 0.0\n",
      "  Tree 2 (weight 1.0):\n",
      "    If (feature 377 <= 54.0)\n",
      "     If (feature 295 <= 253.5)\n",
      "      Predict: 1.0\n",
      "     Else (feature 295 > 253.5)\n",
      "      Predict: 0.0\n",
      "    Else (feature 377 > 54.0)\n",
      "     If (feature 429 <= 11.5)\n",
      "      Predict: 0.0\n",
      "     Else (feature 429 > 11.5)\n",
      "      Predict: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "println(\"Learned classification forest model:\\n\" + rfModelC.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF for Regression\n",
    "\n",
    "Having completed an example of classification with Random Forests, it is time for an example of regression. Once again I will build up on the previous regression example using Decision Trees. \n",
    "\n",
    "The Pipeline for regression, in that case, had only 2 stages - the `featureIndexer` and the `DecisionTreeRegressor`.\n",
    "\n",
    "Now I replace the Decision Tree with the `RandomForestRegressor` and create a new `Pipeline`. This is the `pipelineRFR`, from Random Forest `Regressor`. All the rest is exactly the same as before: calling the `fit` method to get a model and calling the `transform` method to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|            features|     indexedFeatures|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[124,125,126...|(692,[124,125,126...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|(692,[126,127,128...|       0.0|\n",
      "|  0.0|(692,[129,130,131...|(692,[129,130,131...|      0.05|\n",
      "|  0.0|(692,[152,153,154...|(692,[152,153,154...|      0.05|\n",
      "|  0.0|(692,[154,155,156...|(692,[154,155,156...|       0.5|\n",
      "|  0.0|(692,[181,182,183...|(692,[181,182,183...|      0.05|\n",
      "|  1.0|(692,[123,124,125...|(692,[123,124,125...|      0.95|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|      0.95|\n",
      "|  1.0|(692,[124,125,126...|(692,[124,125,126...|       1.0|\n",
      "|  1.0|(692,[125,126,153...|(692,[125,126,153...|      0.95|\n",
      "|  1.0|(692,[128,129,130...|(692,[128,129,130...|      0.95|\n",
      "|  1.0|(692,[150,151,152...|(692,[150,151,152...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|(692,[151,152,153...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|(692,[152,153,154...|      0.95|\n",
      "|  1.0|(692,[155,156,157...|(692,[155,156,157...|       1.0|\n",
      "|  1.0|(692,[155,156,157...|(692,[155,156,157...|       1.0|\n",
      "|  1.0|(692,[156,157,158...|(692,[156,157,158...|       1.0|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|       1.0|\n",
      "|  1.0|(692,[158,159,160...|(692,[158,159,160...|       1.0|\n",
      "|  0.0|(692,[121,122,123...|(692,[121,122,123...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.RandomForestRegressor\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.ml.regression.RandomForestRegressionModel\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mrfR\u001b[39m: \u001b[32mRandomForestRegressor\u001b[39m = rfr_dee04e7012d6\n",
       "\u001b[36mpipelineRFR\u001b[39m: \u001b[32mPipeline\u001b[39m = pipeline_a60a6f283d16\n",
       "\u001b[36mmodelRFR\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mml\u001b[39m.\u001b[32mPipelineModel\u001b[39m = pipeline_a60a6f283d16\n",
       "\u001b[36mpredictions\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [label: double, features: vector ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.regression.RandomForestRegressionModel\n",
    "\n",
    "val rfR = new RandomForestRegressor().setLabelCol(\"label\").setFeaturesCol(\"indexedFeatures\")\n",
    "\n",
    "val pipelineRFR = new Pipeline().setStages(Array(featureIndexer, rfR))\n",
    "\n",
    "val modelRFR = pipelineRFR.fit(toML(trainingData))\n",
    "\n",
    "val predictions = modelRFR.transform(toML(testData))\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF for Regression\n",
    "\n",
    "The predictions are then returned in the `predictionsRFR` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [
      "scala"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|label|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|  0.0|(692,[124,125,126...|\n",
      "|       0.0|  0.0|(692,[126,127,128...|\n",
      "|      0.05|  0.0|(692,[129,130,131...|\n",
      "|      0.05|  0.0|(692,[152,153,154...|\n",
      "|       0.5|  0.0|(692,[154,155,156...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredictionsRFR\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [label: double, features: vector ... 2 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictionsRFR = modelRFR.transform(toML(testData))\n",
    "predictionsRFR.select(\"prediction\", \"label\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Summary\n",
    "\n",
    "Having completed this lesson we should now be able to:\n",
    "\n",
    "* Understand how to run a random forest in Spark\n",
    "* Grasp most of the parameters and their effects \n",
    "* Understand inputs and outputs \n",
    "* Understand how to use Random Forest for regression and categorization\n",
    "\n",
    "### About the Authors\n",
    "\n",
    "[Petro Verkhogliad](https://www.linkedin.com/in/vpetro) is Consulting Manager at Lightbend. He holds a Masters degree in Computer Science with specialization in Intelligent Systems. He is passionate about functional programming and applications of AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12.8",
   "language": "scala",
   "name": "scala_2_12_8"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
